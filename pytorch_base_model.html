<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Quá Trình Huấn Luyện Mạng Nơ-ron</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
        }
        h1 {
            color: #333;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
        code {
            font-family: "Courier New", Courier, monospace;
        }
    </style>
</head>
<body>
    <h1>Các Bước Trong Quá Trình Huấn Luyện Mạng Nơ-ron</h1>
    <ol>
        <li><strong>Đặt lại Gradient:</strong> Sử dụng <code>optimizer.zero_grad()</code> để đặt lại gradient về 0 trước khi tính toán gradient mới.</li>
        <li><strong>Lan Truyền Tiến:</strong> Tính toán đầu ra của mô hình với <code>output = model(data)</code>.</li>
        <li><strong>Tính Toán Hàm Mất Mát:</strong> Sử dụng hàm mất mát để đo lường sự khác biệt giữa dự đoán và nhãn thực tế, ví dụ: <code>loss = loss_func(output, target)</code>.</li>
        <li><strong>Lan Truyền Ngược:</strong> Tính toán gradient của hàm mất mát đối với các tham số của mô hình với <code>loss.backward()</code>.</li>
        <li><strong>Cập Nhật Tham Số:</strong> Sử dụng <code>optimizer.step()</code> để cập nhật các tham số của mô hình dựa trên gradient đã tính toán.</li>
    </ol>

    <h2>Mã Python Minh Họa</h2>
    <pre><code class="language-python">
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# Định nghĩa mô hình đơn giản
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout2d(0.25)
        self.dropout2 = nn.Dropout2d(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output

# Chuẩn bị dữ liệu
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)

# Khởi tạo mô hình, hàm mất mát và optimizer
model = SimpleNN()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

loss_func = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Quá trình huấn luyện
num_epochs = 5
i = 0

for epoch in range(num_epochs):
    for data, target in train_loader:
        data, target = data.to(device), target.to(device)

        # Đặt lại gradient
        optimizer.zero_grad()

        # Lan truyền tiến
        output = model(data)

        # Tính toán hàm mất mát
        loss = loss_func(output, target)

        # Lan truyền ngược
        loss.backward()

        # Cập nhật tham số
        optimizer.step()

        # In thông tin huấn luyện
        if i % 1000 == 0:
            print(f'Train Step: {i} Loss: {loss.item()}')
        i += 1
    </code></pre>
</body>
</html>